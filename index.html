<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?">
<title>Liangcai Su's Personal Homepage</title>
<style>
.author-highlight {
  color: #0066cc;
}
</style>
</head>
<body>
<div id="layout-content">
<div id="toptitle">
<h1>Liangcai Su (苏良才)</h1>
</div>
<table class="imgtable"><tr><td>
<a href="https://liangcaisu.github.io/"><img src="picture/photo.jpg" alt="alt text" width="130px" /></a>&nbsp;</td>
<td align="left"><p>
<!-- <br><br /> -->
Computer Science, Ph.D. Student<br />
The University of Hong Kong<br />
Email: liangcaisu@connect.hku.hk<br />
<br />
<!-- <a href="pdf/Aikun_Xu_CV.pdf">[CV]</a> -->
<a href="https://scholar.google.com/citations?user=h11w7y4AAAAJ&hl=zh-CN">[Google Scholar]</a>
<!-- <a href="https://dblp.org/pid/270/6458.html">[DBLP]</a>  -->
<a href="https://github.com/LiangcaiSu">[GitHub]</a>
<a href="https://www.linkedin.com/in/su-liangcai-1907122b5/">[LinkedIn]</a>
</td></tr></table>

<h2>About Me</h2>
<p> I am a Ph.D. student at The University of Hong Kong, majoring in Computer Science. I am also a founder member of Deep Research Team at <img src="tongyi.png" alt="Tongyi" style="height: 1em; vertical-align: text-top;"> Tongyi Lab. Previously,
I received my Master's degree in Computer Technology from Tsinghua University in 2024 and obtained my B.Eng degree in Software Engineering from Xidian University in 2021.
I was a research intern supervised by <a href="http://jiemingzhu.github.io/">Jieming Zhu</a> and at Tencent mentored by <a target="_blank" rel="noopener"
  href="https://scholar.google.com/citations?user=sUaBkFkAAAAJ&hl=en&oi=ao">Junwei Pan</a> and <a target="_blank" rel="noopener"
  href="https://wxm17.github.io">Ximei Wang</a>.
</p> 


<p> My research directions are Data Mining and Deep Research Agent. </p>

<h2>News</h2>
<ul>
  <li><strong>[Sep. 2025]</strong> We release our open-source and powerful deep research agent <strong>Tongyi DeepResearch</strong>! Please see  <a href="https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/">the tech blog</a> for details. Our model is available at <a href="https://huggingface.co/Alibaba-NLP/Tongyi-DeepResearch-30B-A3B">huggingface</a> and 
  <a href="https://modelscope.cn/models/iic/Tongyi-DeepResearch-30B-A3B">modelscope</a>.</li>
  <li><strong>[Sep. 2025]</strong> We release our work <em>Scaling Agents via Continual Pre-training!</em></li>
  <li><strong>[Aug. 2025]</strong> Our work <em>ImportSnare</em> was accepted by ACM CCS 2025. 🎉</li>
  <li><strong>[Jan. 2025]</strong> One paper about unlearnable data was accepted by ICLR 2025. <span style='font-size:20;'>&#129321;</span>    See you in Singapore!</li>
  <li><strong>[July. 2024]</strong> Our paper ERASE was accepted by CIKM 2024. Congrats to Ling-Hao Chen! </li>
  <!-- <li><strong>[June. 2024]</strong> Our paper (Invisibility Cloak) was accepted by USENIX Security 2024. </li> -->
  <!-- <li><strong>[May. 2024]</strong> Our latest <a href="https://arxiv.org/abs/2405.09592">survey paper</a> on generative techniques for spatial-temporal data mining is now available. </li> -->
  <!-- <li><strong>[Nov. 2023]</strong> Our paper about Multi-model recommendation was accepted by ICASSP 2024. Congrats to Xiaoteng Shen!</li> -->
  <!-- <li><strong>[Nov. 2023]</strong> Our paper about Multi-task recommendation was accepted by AAAI 2024. <span style='font-size:20;'>&#129321;</span>    See you in Vancouver!</li> -->
  <!-- <li><strong>[Apr. 2023]</strong> Our paper about Candidate matching was accepted by SIGIR 2023.</li> -->
  <!-- <li><strong>[Nov. 2022]</strong> One paper about CTR prediction was accepted by AAAI 2023.</li> -->
  <!-- <li><strong>[Apr. 2022]</strong> I was selected for "Tencent Rhino-Bird Research Elite Program 2022".</li> -->
  <!-- <li><strong>[Apr. 2022]</strong> One paper about Benchmarking for recommender systems  was accepted by SIGIR 2022.</li> -->
  <!-- <li><strong>[Jan. 2022]</strong> One paper about Re-ranking was accepted by WWW 2022.</li> -->
</ul>



<h2>Research Experiences</h2>
<ul>
  <li><strong>Tongyi Lab (Deep Research Team)</strong>, Research Intern (Founder Member). May. 2025 - </li>
  <li><strong>The University of Hong Kong</strong>, Research Assistant. Nov. 2023 - Aug. 2024</li>
  <li><strong>Tencent</strong>, Research Intern. Jun. 2022 - Nov. 2023</li>
  <!-- <li><strong>Huawei Noah's Ark Lab</strong>, Research Intern. Mar. 2021 - Sep. 2021 & Feb. 2022 - May. 2022.</li> -->
</ul>


<h2>Publications</h2>
(* denotes equal contributions)

<ul>
  <li><p><strong>Tongyi-DeepResearch</strong><br />
    DeepResearch Team, Tongyi Lab<br />
    Arxiv 2025. <a href="https://tongyi-agent.github.io/">[📰 Blog]</a>
    <a href="https://huggingface.co/Alibaba-NLP/Tongyi-DeepResearch-30B-A3B">[🤗 HuggingFace]</a>
    <a href="https://modelscope.cn/models/iic/Tongyi-DeepResearch-30B-A3B">[<img src="modelscope.svg" alt="MS" style="height: 0.9em; vertical-align: middle; display: inline;"> ModelScope]</a>
    <a href="https://github.com/Alibaba-NLP/DeepResearch/">[<svg style="height: 1em; vertical-align: middle; display: inline;" viewBox="0 0 16 16" fill="currentColor"><path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path></svg> GitHub]</a>
  </p>
  </li>

  <li><p><strong>Scaling Agents via Continual Pre-training</strong><br />
    <span class="author-highlight"><strong>Liangcai Su*</strong></span>, Zhen Zhang*, Guangyu Li*, Zhuo Chen*, Chenxi Wang*, Maojia Song, Xinyu Wang, et al.<br />
    Arxiv 2025. <a href="https://arxiv.org/abs/2509.13310">[📄 Paper]</a>
    <a href="https://venturebeat.com/ai/build-research-agents-without-api-costs-alibabas-offline-data-synthesis">[📰 VentureBeat]</a>
  </p>
  </li>

  <li><p><strong>Towards General Agentic Intelligence via Environment Scaling</strong><br />
    Runnan Fang, Shihao Cai, Baixuan Li, Jialong Wu, Guangyu Li, Wenbiao Yin, Xinyu Wang, Xiaobin Wang,<br />
    <span class="author-highlight"><strong>Liangcai Su</strong></span>, Zhen Zhang, et al.<br />
    Arxiv 2025. <a href="https://arxiv.org/abs/2509.13311">[📄 Paper]</a></p>
  </li>

  <li><p><strong>R1-Fuzz: Specializing Language Models for Textual Fuzzing via Reinforcement Learning</strong><br />
    Jiayi Lin, <span class="author-highlight"><strong>Liangcai Su</strong></span>, Junzhe Li, Chenxiong Qian.<br />
    Arxiv 2025. <a href="https://arxiv.org/abs/2509.20384">[📄 Paper]</a>
    <a href="https://github.com/HKU-System-Security-Lab/R1-Fuzz">[<svg style="height: 1em; vertical-align: middle; display: inline;" viewBox="0 0 16 16" fill="currentColor"><path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path></svg> GitHub]</a>
    <a href="https://huggingface.co/0gal1/R1-Fuzz-7B">[🤗 HuggingFace]</a></p>
  </li>

  <li><p><strong>ImportSnare: Directed "Code Manual" Hijacking in Retrieval-Augmented Code Generation</strong><br />
    Kai Ye, <span class="author-highlight"><strong>Liangcai Su</strong></span>, Chenxiong Qian.<br />
    ACM CCS 2025. <a href="https://arxiv.org/abs/2509.07941">[📄 Paper]</a><a href="https://importsnare.github.io/">[🏠 Homepage]</a></p>
  </li>
  <li><p><strong>How Far Are We from True Unlearnability?</strong><br />
    Kai Ye*, <span class="author-highlight"><strong>Liangcai Su*</strong></span>, Chenxiong Qian.<br />
        ICLR 2025. <a href="https://arxiv.org/abs/2509.08058">[📄 Paper]</a></p>
  </li>
  <li><p><strong>STEM: Unleashing the Power of Embeddings for Multi-task Recommendation</strong> <br />
    <span class="author-highlight"><strong>Liangcai Su*</strong></span>, Junwei Pan*, Ximei Wang, Xi Xiao, Shijie Quan, Xihua Chen, Jie Jiang. <br />AAAI 2024. <a href="https://arxiv.org/abs/2308.13537">[📄 Paper]</a> <a href="https://github.com/LiangcaiSu/STEM">[<svg style="height: 1em; vertical-align: middle; display: inline;" viewBox="0 0 16 16" fill="currentColor"><path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path></svg> GitHub]</a>  </p>
      </li>
</ul>

<div id="more-publications" style="display: none;">
<ul>
  <li><p><strong>ERASE: Error-Resilient Representation Learning on Graphs for Label Noise Tolerance</strong><br />
    Ling-Hao Chen, Yuanshuo Zhang, Taohua Huang, <span class="author-highlight"><strong>Liangcai Su</strong></span>, Zeyi Lin, Xi Xiao, Xiaobo Xia, Tongliang Liu.<br />
    CIKM 2024. <a href="https://arxiv.org/pdf/2312.08852">[📄 Paper]</a><a href="https://eraseai.github.io/ERASE-page">[🏠 Homepage]</a><a href="https://github.com/eraseai/erase">[<svg style="height: 1em; vertical-align: middle; display: inline;" viewBox="0 0 16 16" fill="currentColor"><path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path></svg> GitHub]</a></p>
  </li>
  <li><p><strong>Invisibility Cloak: Proactive Defense Against Visual Game Cheating</strong><br />
    Chenxin Sun, Kai Ye, <span class="author-highlight"><strong>Liangcai Su</strong></span>, Jiayi Zhang, and Chenxiong Qian.<br />
    USENIX Security 2024. <a href="https://www.usenix.org/conference/usenixsecurity24/presentation/sun-chenxin">[📄 Paper]</a> <a href="https://inviscloak.github.io/">[🏠 Homepage]</a>  </p>
  </li>
  <li><p><strong>A Survey of Generative Techniques for Spatial-Temporal Data Mining</strong><br />
    Qianru Zhang, Haixin Wang, Cheng Long, <span class="author-highlight"><strong>Liangcai Su</strong></span>, Xingwei He, Jianlong Chang, Tailin Wu, Hongzhi Yin, Siu-Ming Yiu, Qi Tian and Christian S. Jensen.<br />
        Preprint. <a href="https://arxiv.org/abs/2405.09592">[📄 Paper]</a>  </p>
  </li>
</ul>
</div>

<ul>
  <li><p><strong>Multi-interest Learning for Multi-modal Paper Recommendation</strong><br />
    Xiaoteng Shen*, <span class="author-highlight"><strong>Liangcai Su*</strong></span>, Xi Xiao, Yi Li<br />
        ICASSP 2024. <a href="https://ieeexplore.ieee.org/abstract/document/10446181">[📄 Paper]</a>  </p>
  </li>
  <li><p><strong>Beyond Two-Tower Matching: Learning Sparse Retrievable Interaction Models for Recommendation</strong><br />
    <span class="author-highlight"><strong>Liangcai Su</strong></span>, Fan Yan, Jieming Zhu, Xi Xiao, Haoyi Duan, Zhou Zhao, Zhenhua Dong and Ruiming Tang.<br />
        SIGIR 2023. <a href="https://dl.acm.org/doi/abs/10.1145/3539618.3591643">[📄 Paper]</a>  </p>
      </li>
</ul>

<div id="more-publications-2" style="display: none;">
<ul>
  <li><p><strong>FinalMLP: An Enhanced Two-Stream MLP Model for CTR Prediction</strong><br />
    Kelong Mao, Jieming Zhu, <span class="author-highlight"><strong>Liangcai Su</strong></span>, Guohao Cai, Yuru Li, Zhenhua Dong.<br />
    AAAI 2023. <a href="https://arxiv.org/pdf/2304.00902.pdf">[📄 Paper]</a> <a href="https://github.com/reczoo/FuxiCTR/tree/main/model_zoo/FinalMLP">[<svg style="height: 1em; vertical-align: middle; display: inline;" viewBox="0 0 16 16" fill="currentColor"><path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path></svg> GitHub]</a> </p>
  </li>
  <li><p><strong>BARS: Towards Open Benchmarking for Recommender Systems</strong><br />
    Jieming Zhu, Quanyu Dai, <span class="author-highlight"><strong>Liangcai Su</strong></span>, Rong Ma, Jinyang Liu, Guohao Cai, Xi Xiao, Rui Zhang. <br />
    SIGIR 2022. <a href="https://arxiv.org/abs/2205.09626">[📄 Paper]</a> <a href="https://github.com/openbenchmark/BARS">[<svg style="height: 1em; vertical-align: middle; display: inline;" viewBox="0 0 16 16" fill="currentColor"><path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path></svg> GitHub]</a> <a href="https://openbenchmark.github.io/BARS/">[🏠 Homepage]</a> </p>
  </li>
  <li><p><strong>PEAR: Personalized Re-ranking with Contextualized Transformer for Recommendation</strong> <br />
    Yi Li, Jieming Zhu, Weiwen Liu, <span class="author-highlight"><strong>Liangcai Su</strong></span>, Guohao Cai, Qi Zhang, Ruiming Tang, Xi Xiao, Xiuqiang He. <br />
    WWW 2022. <a href="https://dl.acm.org/doi/10.1145/3487553.3524208">[📄 Paper]</a>
  </p></li>
</ul>
</div>

<p><a href="javascript:void(0);" onclick="togglePublications()" id="toggle-link">[Show all publications]</a></p>

<script>
function togglePublications() {
  var morePublications = document.getElementById("more-publications");
  var morePublications2 = document.getElementById("more-publications-2");
  var toggleLink = document.getElementById("toggle-link");

  if (morePublications.style.display === "none") {
    morePublications.style.display = "block";
    morePublications2.style.display = "block";
    toggleLink.textContent = "[Show less]";
  } else {
    morePublications.style.display = "none";
    morePublications2.style.display = "none";
    toggleLink.textContent = "[Show all publications]";
  }
}
</script>

<!-- <h2>Preprints</h2> -->


<h2>Awards</h2>
<ul>
  <li>Outstanding Graduates of Beijing(北京市优秀毕业生), 2024. </li>
  <li>Best Report Award, The 15th Nanshan Academic Forum for Doctoral Students in the Guangdong-Hong Kong-Macao Greater Bay Area, 2023. </li>
  <li>ACM SIGIR 2023 Student Travel Award. </li>
  <li>Comprehensive Excellent Second-class Scholarship, Tsinghua University, 2022 & 2023 (Two times).</li>
  <li>National Scholarship(国家奖学金), 2019-2020. (Top 1% Student in XDU) </li>
  <li>National Scholarship(国家奖学金), 2017-2018. (Top 1% Student in XDU) </li>
</ul>


<div id="footer">
<div id="footer-text">
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-VGMZSKJ2WQ"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-VGMZSKJ2WQ');
</script>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?a4e08f102e0496f04fabcc2686260f37";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

</div>
</div>
</div>
</body>
</html>
